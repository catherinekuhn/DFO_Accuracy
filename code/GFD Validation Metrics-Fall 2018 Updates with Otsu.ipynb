{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Flood Validation Metrics Sept 2018 Updates with Otsu\n",
    "Written in Python 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook was built in Catherine's vectorenv\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "#import pylabb\n",
    "import matplotlib as plt\n",
    "import datetime as datetime  \n",
    "import statsmodels.api as sm  \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ckuhn/Downloads\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>B4</th>\n",
       "      <th>B5</th>\n",
       "      <th>B7</th>\n",
       "      <th>MNDWI</th>\n",
       "      <th>NDVI</th>\n",
       "      <th>analyst</th>\n",
       "      <th>date</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel_lon</th>\n",
       "      <th>point_lat</th>\n",
       "      <th>point_lon</th>\n",
       "      <th>random</th>\n",
       "      <th>std_2day</th>\n",
       "      <th>std_3day</th>\n",
       "      <th>otsu_2day</th>\n",
       "      <th>otsu_3day</th>\n",
       "      <th>strata</th>\n",
       "      <th>validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>792.15651</td>\n",
       "      <td>896.579047</td>\n",
       "      <td>761.223987</td>\n",
       "      <td>1833.576595</td>\n",
       "      <td>980.842213</td>\n",
       "      <td>516.20095</td>\n",
       "      <td>-0.338543</td>\n",
       "      <td>0.408565</td>\n",
       "      <td>SiL</td>\n",
       "      <td>1.534720e+12</td>\n",
       "      <td>...</td>\n",
       "      <td>87.277107</td>\n",
       "      <td>25.681778</td>\n",
       "      <td>87.277107</td>\n",
       "      <td>0.853473</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          B1          B2          B3           B4          B5         B7  \\\n",
       "0  792.15651  896.579047  761.223987  1833.576595  980.842213  516.20095   \n",
       "\n",
       "      MNDWI      NDVI analyst          date     ...      pixel_lon  point_lat  \\\n",
       "0 -0.338543  0.408565     SiL  1.534720e+12     ...      87.277107  25.681778   \n",
       "\n",
       "   point_lon    random  std_2day  std_3day  otsu_2day  otsu_3day  strata  \\\n",
       "0  87.277107  0.853473       0.0       0.0        0.0        0.0       0   \n",
       "\n",
       "   validation  \n",
       "0         0.0  \n",
       "\n",
       "[1 rows x 24 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /Users/ckuhn/Downloads/\n",
    "\n",
    "raw = pd.read_csv('gfdValidationPoints_10_02_18.csv',index_col=None, header=0)\n",
    "raw = raw.drop(['system:index','.geo'], axis=1)\n",
    "raw.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of unique floods: 123\n",
      "no of validation points: 34141\n",
      "representative breakdown:\n",
      "no of dry points: 8320\n",
      "no of permanent water points: 8352\n",
      "no of flooded points: 17465\n"
     ]
    }
   ],
   "source": [
    "# Validation** = student classification  \n",
    "# Strata** = DFO classification\n",
    "\n",
    "print('no. of unique floods:', len(raw['dfoID'].unique()))\n",
    "print('no of validation points:', len(raw))\n",
    "\n",
    "print('representative breakdown:')\n",
    "print('no of dry points:', len(raw.loc[raw['std_2day'] == 0]))\n",
    "print('no of permanent water points:', len(raw.loc[raw['std_2day'] == 1]))\n",
    "print('no of flooded points:', len(raw.loc[raw['std_2day'] == 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop floods with few points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw.groupby('dfoID').filter(lambda g: g.dfoID.count() >= 200)\n",
    "#df.dfoID.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop NA points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pixels got labeled \"NAN\" were coded as either -99 or 99 \n",
    "df = df.loc[df['validation'] > -1]\n",
    "df = df.loc[df['validation'] < 99]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collections \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The points were stratified into three classes or strata. They are 50% not flooded (permanent water + dry) and 50% flooded. \n",
    "- not flooded/dry (0)\n",
    "- permanent water (1)\n",
    "- flooded (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doing one method at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_score\n",
      "0.0     7343\n",
      "1.0     7475\n",
      "2.0    15926\n",
      "Name: dfoID, dtype: int64\n",
      "student_score\n",
      "0.0    11653\n",
      "1.0    19091\n",
      "Name: dfoID, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# # Select which collection to analyze: \n",
    "# They all have the same number of floods\n",
    "st2 = df.filter(['dfoID', 'std_2day','strata', 'validation' ], axis=1)\n",
    "st3 = df.filter(['dfoID', 'std_3day','strata', 'validation' ], axis=1)\n",
    "ot2 = df.filter(['dfoID', 'otsu_2day','strata', 'validation' ], axis=1)\n",
    "ot3 = df.filter(['dfoID', 'otsu_3day','strata', 'validation' ], axis=1)\n",
    "\n",
    "# Create a label for the method to use in filenaming\n",
    "method = st2\n",
    "method_name = 'std_2day'\n",
    "\n",
    "method['model_score'] = method[method_name]\n",
    "method['student_score'] = method['validation']\n",
    "\n",
    "print(method.groupby('model_score')['dfoID'].count())\n",
    "# strata = 0 = dry, 1 = permanent water, 2 = flood\n",
    "print(method.groupby(['student_score'])['dfoID'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop NaN values (-99, 99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Inputs for Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate matrix components\n",
    "\n",
    "\n",
    "- true positive = Validation (1) + Strata(4) = 5\n",
    "- true negative = Validation (0) + Strata (0) =  0 \n",
    "- false positive = Validation (0) + Strata (4) = 4\n",
    "- false negative = Validation (1) + Strata (0) = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate aggregate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives(%) 0.3002277708539258\n",
      "True Positives(%) 0.5174695947397825\n",
      "False Positives(%) 0.16696033349091066\n",
      "False Negatives(%) 0.015342300915380979\n",
      "23269\n",
      "0.8176973655937083\n",
      "0.24394072585708904\n",
      "0.028794966930150025\n"
     ]
    }
   ],
   "source": [
    "method['model_score'] = method['model_score'].multiply(2)\n",
    "method['model_score'].value_counts()\n",
    "# 0 - dry, 2 - permanent water - 4 - floods\n",
    "# 0 - dry, 1 - flooded\n",
    "method['student_score'].value_counts()\n",
    "# Add them together\n",
    "method['score']  = method['model_score'] + method['student_score'] \n",
    "method['score'].value_counts()\n",
    "\n",
    "# True Positive = Validation (1) + Strata (4)\n",
    "tp = len(method.loc[method['score']== 5])\n",
    "# True Negative = Validation (0) + Strata (0)\n",
    "tn = len(method.loc[method['score']== 0])\n",
    "# False Positive = Validation (0) + Strata (4)\n",
    "fp = len(method.loc[method['score']== 4])\n",
    "# False Negative = Validation (1) + Strata (0)\n",
    "fn = len(method.loc[method['score']== 1])\n",
    "\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "recall  = tp / (tp + fn)  # % of flooded pixels it ID'ed correctly\n",
    "accuracy = (tn + tp) / (tn+tp+fp+fn)\n",
    "commission = fp / (tp + fp)  # modeled positives over all true positives\n",
    "omission = fn / (tp + fn)\n",
    "\n",
    "total = tn+tp+fn+fp\n",
    "print('True Negatives(%)',tn/total) #32   Sep 2018 updates: 30%\n",
    "print('True Positives(%)',tp/total) #52   Sep 2018 updates: 52%\n",
    "print('False Positives(%)',fp/total) #15  Sep 2018 updates: 16%\n",
    "print('False Negatives(%)',fn/total) # <0.01  Sep 2018 updates: 1.6%\n",
    "print(total)\n",
    "print(accuracy)\n",
    "print(commission)\n",
    "print(omission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_names =  ['Method', 'tp', 'tn','fp','fn',\n",
    "             'precision','recall','overall_accuracy',\n",
    "             'commission','omission'] ### Column names for making data frames for each flood\n",
    "my_df  = pd.DataFrame(columns = col_names)## Make empty data frame to put values for each sampling level in\n",
    "\n",
    "#std_2day = [method_name, tp, tn, fp, fn, precision, recall, accuracy, commission, omission]\n",
    "#std_3day = [method_name, tp, tn, fp, fn, precision, recall, accuracy, commission, omission]\n",
    "#otsu_2day = [method_name, tp, tn, fp, fn, precision, recall, accuracy, commission, omission]\n",
    "otsu_3day = [method_name, tp, tn, fp, fn, precision, recall, accuracy, commission, omission]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#std2  = pd.DataFrame([std_2day], columns = col_names)## Make empty data frame to put values for each sampling level in\n",
    "#std3  = pd.DataFrame([std_3day], columns = col_names)## Make empty data frame to put values for each sampling level in\n",
    "#ot2  = pd.DataFrame([otsu_2day], columns = col_names)## Make empty data frame to put values for each sampling level in\n",
    "ot3  = pd.DataFrame([otsu_3day], columns = col_names)## Make empty data frame to put values for each sampling level in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "win = std2.append(std3).append(ot2).append(ot3)\n",
    "win"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to make a loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda2/envs/vectorenv/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3077\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3078\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3079\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'score'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-6078d5a1abe1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msampling_levels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m## Loop through sampling levels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m##Apparently pandas has a built-in dataframe random sampler!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mtp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mtn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/vectorenv/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2688\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/vectorenv/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2693\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2695\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/vectorenv/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2487\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2489\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2490\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2491\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/vectorenv/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4115\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4116\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/vectorenv/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3078\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3082\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'score'"
     ]
    }
   ],
   "source": [
    "df = st2 # st2, st3, ot2, ot3\n",
    "df['score'] = df[ method_name]\n",
    "\n",
    "floods = list(set(df.dfoID)) ## Get a list of all the unique values of floods\n",
    "sampling_levels = np.arange(0, 200, 1) # list(set(df.method))\n",
    "\n",
    "col_names =  ['Flood','NumofPoints', 'tp', 'tn','fp','fn',\n",
    "             'precision','recall','overall_accuracy',\n",
    "             'commission','omission'] ### Column names for making data frames for each flood\n",
    "\n",
    "my_df  = pd.DataFrame(columns = col_names)## Make empty data frame to put values for each sampling level in\n",
    "\n",
    "for i in floods:  ### Loop through floods\n",
    "    df_full = raw.loc[raw['dfoID']== i] ### subset data frame by single flood\n",
    "    for j in sampling_levels: ## Loop through sampling levels\n",
    "        df = df_full.sample(j) ##Apparently pandas has a built-in dataframe random sampler!\n",
    "        tp = float(len(df.loc[df['score']== 5])) \n",
    "        tn = float(len(df.loc[df['score']== 0]))\n",
    "        fp = float(len(df.loc[df['score']== 4]))\n",
    "        fn = float(len(df.loc[df['score']== 1]))\n",
    "        ### Calculating values!\n",
    "        if tp == 0: ###Hmm, sometimes there are no tp scores, so you get a zero divide error\n",
    "            continue ###I've put this in for now but it's gonna bite your butt eventually\n",
    "        precision = tp / (tp + fp)\n",
    "        recall  = tp / (tp + fn)  # % of flooded pixels it ID'ed correctly\n",
    "        accuracy = (tn + tp) / (tn+tp+fp+fn)\n",
    "        commission = fp / (tp + fp)  # modeled positives over all true positives\n",
    "        omission = fn / (tp + fn)\n",
    "        ##add a new row to the dataframe based on this sampling level!\n",
    "        my_df.loc[len(my_df)] = [i,j, tp, tn, fp, fn, precision, recall, accuracy, commission, omission]\n",
    "\n",
    "        ###Now you should have a full dataframe with all your sampling levels for each flood\n",
    "        ###SO\n",
    "        \n",
    "#Export Results\n",
    "#%cd '/Users/ckuhn/Desktop/gfd_accuracy_rename/lumped/'\n",
    "#Anna's original - my_df.to_csv('myprecious.csv', encoding='utf-8')\n",
    "#my_df.to_csv('gfd_summary_validaton_metrics_resampled.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Flood</th>\n",
       "      <th>NumofPoints</th>\n",
       "      <th>tp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>overall_accuracy</th>\n",
       "      <th>commission</th>\n",
       "      <th>omission</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Flood, NumofPoints, tp, tn, fp, fn, precision, recall, overall_accuracy, commission, omission]\n",
       "Index: []"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a new dataframe with values for each method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def set_style():\n",
    "#     plt.style.use(['seaborn-white', 'seaborn-talk'])\n",
    "#     plt.rc(\"font\", family=\"Times New Roman\", size = 50)\n",
    "    \n",
    "# set_style()  \n",
    "\n",
    "# def plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "#     plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "#     plt.title(title)\n",
    "#     plt.colorbar()\n",
    "#     tick_marks = np.arange(len(target_names))\n",
    "#     plt.xticks(tick_marks, target_names, rotation=45)\n",
    "#     plt.yticks(tick_marks, target_names)\n",
    "#     plt.tight_layout()\n",
    "\n",
    "#     width, height = cm.shape\n",
    "\n",
    "#     for x in range(width):\n",
    "#         for y in range(height):\n",
    "#             plt.annotate(str(cm[x][y]), xy=(y, x), \n",
    "#                         horizontalalignment='center',\n",
    "#                         verticalalignment='center')\n",
    "#     plt.ylabel('True label', fontsize=24)\n",
    "#     plt.xlabel('Predicted label',  fontsize=24)\n",
    "\n",
    "#     tp,\n",
    "# cm = np.array([[tp, fn],[fp, tn]])\n",
    "# plot_confusion_matrix(cm, ['Flooded', 'Not Flooded'])\n",
    "\n",
    "# #%cd /Users/ckuhn/Desktop/gfd_accuracy_rename/figures\n",
    "# #plt.savefig('GFD_overall_confusion_matrix_Sep2018.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Summarized Error Metrics By Flood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "floods = list(set(raw.dfoID)) ## Get a list of all the unique values of floods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# floods = list(set(raw.dfoID)) ## Get a list of all the unique values of floods\n",
    "# sampling_levels = np.arange(0, 200, 1)\n",
    "\n",
    "# col_names =  ['Flood','Method', 'NumofPoints', 'tp', 'tn','fp','fn',\n",
    "#              'precision','recall','overall_accuracy',\n",
    "#              'commission','omission'] ### Column names for making data frames for each flood\n",
    "# my_df  = pd.DataFrame(columns = col_names)## Make empty data frame to put values for each sampling level in\n",
    "\n",
    "# for i in floods:  ### Loop through floods\n",
    "#     df_full = raw.loc[raw['dfoID']== i] ### subset data frame by single flood\n",
    "#     for j in sampling_levels: ## Loop through sampling levels\n",
    "#         df = df_full.sample(j) ##Apparently pandas has a built-in dataframe random sampler!\n",
    "#         tp = float(len(df.loc[df['score']== 5])) \n",
    "#         tn = float(len(df.loc[df['score']== 0]))\n",
    "#         fp = float(len(df.loc[df['score']== 4]))\n",
    "#         fn = float(len(df.loc[df['score']== 1]))\n",
    "#         ### Calculating values!\n",
    "#         if tp == 0: ###Hmm, sometimes there are no tp scores, so you get a zero divide error\n",
    "#             continue ###I've put this in for now but it's gonna bite your butt eventually\n",
    "#         precision = tp / (tp + fp)\n",
    "#         recall  = tp / (tp + fn)  # % of flooded pixels it ID'ed correctly\n",
    "#         accuracy = (tn + tp) / (tn+tp+fp+fn)\n",
    "#         commission = fp / (tp + fp)  # modeled positives over all true positives\n",
    "#         omission = fn / (tp + fn)\n",
    "#         ##add a new row to the dataframe based on this sampling level!\n",
    "#         my_df.loc[len(my_df)] = [i,j, tp, tn, fp, fn, precision, recall, accuracy, commission, omission]\n",
    "\n",
    "#         ###Now you should have a full dataframe with all your sampling levels for each flood\n",
    "#         ###SO\n",
    "        \n",
    "# #Export Results\n",
    "# #%cd '/Users/ckuhn/Desktop/gfd_accuracy_rename/lumped/'\n",
    "# #Anna's original - my_df.to_csv('myprecious.csv', encoding='utf-8')\n",
    "# #my_df.to_csv('gfd_summary_validaton_metrics_resampled.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "floods = list(set(raw.dfoID)) ## Get a list of all the unique values of floods\n",
    "#sampling_levels = np.arange(0, 200, 1)\n",
    "\n",
    "col_names =  ['Flood','NumofPoints', 'tp', 'tn','fp','fn',\n",
    "             'precision','recall','overall_accuracy',\n",
    "             'commission','omission'] ### Column names for making data frames for each flood\n",
    "\n",
    "my_df  = pd.DataFrame(columns = col_names)## Make empty data frame to put values for each sampling level in\n",
    "\n",
    "for i in floods:  ### Loop through floods\n",
    "    for col in df.columns[18:22]:\n",
    "    \n",
    "    df = df_full.loc(j) ##Apparently pandas has a built-in dataframe random sampler!\n",
    "    df = raw.loc[raw['dfoID']== i] ### subset data frame by single flood\n",
    "    tp = float(len(df.loc[df['score']== 5])) \n",
    "    tn = float(len(df.loc[df['score']== 0]))\n",
    "    fp = float(len(df.loc[df['score']== 4]))\n",
    "    fn = float(len(df.loc[df['score']== 1]))\n",
    "        ### Calculating values!\n",
    "    if tp == 0: ###Hmm, sometimes there are no tp scores, so you get a zero divide error\n",
    "        continue ###I've put this in for now but it's gonna bite your butt eventually\n",
    "    precision = tp / (tp + fp)\n",
    "    recall  = tp / (tp + fn)  # % of flooded pixels it ID'ed correctly\n",
    "    accuracy = (tn + tp) / (tn+tp+fp+fn)\n",
    "    commission = fp / (tp + fp)  # modeled positives over all true positives\n",
    "    omission = fn / (tp + fn)\n",
    "    ##add a new row to the dataframe based on this sampling level!\n",
    "    my_df.loc[len(my_df)] = [i,col, tp, tn, fp, fn, precision, recall, accuracy, commission, omission]\n",
    "\n",
    "        ###Now you should have a full dataframe with all your sampling levels for each flood\n",
    "        ###SO\n",
    "        \n",
    "#Export Results\n",
    "#%cd '/Users/ckuhn/Desktop/gfd_accuracy_rename/lumped/'\n",
    "#Anna's original - my_df.to_csv('myprecious.csv', encoding='utf-8')\n",
    "#my_df.to_csv('gfd_summary_validaton_metrics_resampled.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-set resampled floods to just the ones with 200 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the dataframes with 200 points\n",
    "my_df = my_df.loc[my_df['NumofPoints'] == 199]\n",
    "\n",
    "# Drop the one garbage dataframe\n",
    "#my_df = my_df.loc[my_df['Flood'] != 'SiL']\n",
    "\n",
    "len(my_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recast as an integer\n",
    "my_df['Flood'] = my_df['Flood'].astype(int)\n",
    "\n",
    "# Export results\n",
    "%cd '/Users/ckuhn/Desktop/gfd_accuracy_rename/lumped/'\n",
    "my_df.to_csv('gfd_summary_validaton_metrics.csv', encoding='utf-8')  #aka my_df.to_csv('myprecious3.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep Data for Export to Long Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prepped = my_df.copy()     \n",
    "prepped.rename(columns={'tp': 'True Positive', 'fp': 'False Positive', 'fn':'False Negative', 'tn': 'True Negative', 'overall_accuracy':'Overall Accuracy', 'commission':'Commission', 'omission':'Omission', 'precision':'Precision', 'recall':'Recall'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmelted = prepped.melt(id_vars=['Flood', 'NumofPoints'], var_name='Metric')\n",
    "dfmelted.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results\n",
    "%cd '/Users/ckuhn/Desktop/gfd_accuracy_rename/lumped/'\n",
    "dfmelted.to_csv('gfd_summary_validaton_metrics_longform.csv', encoding='utf-8')  #aka my_df.to_csv('myprecious3.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 59 floods\n",
    "\n",
    "print('% of Floods with >90% accuracy', (len(my_df.loc[my_df['overall_accuracy'] >= 0.9]))/len(my_df))\n",
    "print('% of Floods with >75% accuracy', (len(my_df.loc[my_df['overall_accuracy'] >= 0.75]))/len(my_df))\n",
    "my_df['overall_accuracy'].describe()\n",
    "\n",
    "# For 59 floods, mean accuracy is 82%, median accuracy is 86%, 75% of floods have 94% accuracy\n",
    "    # 41% of the floods have 90% or better accuracy \n",
    "    # 73% of the flood have 80% or better accuracy \n",
    "# for 37 floods:  mean accuracy is 85%, median accuracy is 89%, 75% of the floods have 92% accuracy. \n",
    "    # and 49% of the floods have 90% or better accuracy \n",
    "    # 78% of the flood have 80% or better accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors of Commission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df['commission'].mean()\n",
    "\n",
    "#From 37 floods: 0.22182899539761314\n",
    "#From 59 floods: 0.25010345838907294"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'axes.titlesize':'32',\n",
    "          'xtick.labelsize':'24',\n",
    "          'ytick.labelsize':'24'}\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "# Go back to normal\n",
    "#matplotlib.rcParams.update(matplotlib.rcParamsDefault)  # to revert to default settings\n",
    "\n",
    "\n",
    "my_df['commission'].hist(color='grey')\n",
    "plt.ylabel('count', fontsize=24)\n",
    "plt.xlabel('Rate of Commission Errors', fontsize=24)\n",
    "\n",
    "plt.axvline(x=0.22182899539761314, color='black', ls='--')\n",
    "plt.axvline(x=0.25010345838907294, color='red', ls='--')\n",
    "\n",
    "# mean errors of commission is 25%, \n",
    "\n",
    "#%cd /Users/ckuhn/Desktop/gfd_accuracy_rename/figures/\n",
    "#plt.savefig('GFD_overall_commission.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors of Omission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df['omission'].mean()\n",
    "\n",
    "#From 37 floods: 0.02238782782112279\n",
    "#From 59 floods: 0.022641702349266005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df['omission'].hist(color='grey')\n",
    "plt.ylabel('count', fontsize=24)\n",
    "plt.xlabel('Errors of Omission', fontsize=24)\n",
    "plt.axvline(x=0.02238782782112279, color='black', ls='--')\n",
    "plt.axvline(x=0.022641702349266005, color='red', ls='--')\n",
    "\n",
    "%cd /Users/ckuhn/Desktop/gfd_accuracy_rename/figures\n",
    "plt.savefig('GFD_overall_omission.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_style():\n",
    "    plt.style.use(['seaborn-white', 'seaborn-talk'])\n",
    "    plt.rc(\"font\", family=\"Times New Roman\", size = 22)\n",
    "    plt.rc('xtick', labelsize=22) \n",
    "    plt.rc('ytick', labelsize=22) \n",
    "set_style()  \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,7))\n",
    "\n",
    "my_df.groupby(['Flood'])['overall_accuracy'].mean().plot(kind='bar', color='grey')\n",
    "plt.ylabel('Overall Accuracy', fontsize = 30)\n",
    "plt.xlabel('DFO ID', fontsize = 30)\n",
    "plt.axhline(y=0.9, color='red', ls='--')\n",
    "plt.axhline(y=0.75, color='#FFA07A', ls='--')\n",
    "\n",
    "plt.xticks(rotation=60)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "%cd /Users/ckuhn/Desktop/gfd_accuracy_rename/figures/\n",
    "plt.savefig('GFD_overall_accuracy_by_flood.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_style():\n",
    "    plt.style.use(['seaborn-white', 'seaborn-talk'])\n",
    "    plt.rc(\"font\", family=\"Times New Roman\", size = 10)\n",
    "    plt.rc('xtick', labelsize=10) \n",
    "    plt.rc('ytick', labelsize=10) \n",
    "set_style()  \n",
    "\n",
    "plt.figure(figsize=(40, 12))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(20,7))\n",
    "\n",
    "my_df.groupby(['Flood'])['tp'].mean().plot(kind='bar', color = \n",
    "                                                              ['grey', 'black'], ax = ax[0,0])\n",
    "ax[0, 0].set_ylabel('True Positives')\n",
    "ax[0, 0].set_xlabel('')\n",
    "ax[0, 0].set_xticklabels([])\n",
    "\n",
    "my_df.groupby(['Flood'])['tn'].mean().plot(kind='bar', color = 'grey', ax=ax[0, 1])\n",
    "ax[0, 1].set_ylabel('True Negatives')\n",
    "ax[0, 1].set_xlabel('')\n",
    "ax[0, 1].set_xticklabels([])\n",
    "\n",
    "my_df.groupby(['Flood'])['fp'].mean().plot(kind='bar', color = 'grey', ax=ax[1, 0])\n",
    "ax[1, 0].set_ylabel('False Positives')\n",
    "ax[1, 0].set_xlabel('DFO ID')\n",
    "\n",
    "my_df.groupby(['Flood'])['fn'].mean().plot(kind='bar', color = 'grey', ax=ax[1, 1])\n",
    "ax[1, 1].set_ylabel('False Negatives')\n",
    "ax[1, 1].set_xlabel('DFO ID')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "%cd /Users/ckuhn/Desktop/gfd_accuracy_rename/figures/\n",
    "plt.savefig('GFD_base_metrics_by_flood.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /Users/ckuhn/Desktop/gfd_accuracy_rename/lumped/\n",
    "test = pd.read_csv('gfd_summary_validaton_metrics_longform.csv', low_memory=False)\n",
    "test.groupby(['Metric'])['value'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error = test.loc[test['Metric'].isin(['Commission','Omission'])]\n",
    "overall = test.loc[test['Metric'].isin(['Overall Accuracy','Precision','Recall' ])]\n",
    "basics = test.loc[test['Metric'].isin(['True Positive','True Negative','False Positive', 'False Negative' ])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.Metric.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_style():\n",
    "    plt.style.use(['seaborn-white', 'seaborn-talk'])\n",
    "    plt.rc(\"font\", family=\"Times New Roman\", size = 24)\n",
    "    \n",
    "set_style()    \n",
    "\n",
    "ax = sns.barplot(y=\"value\", x=\"Metric\", palette= ['#388e3c', '#6abf69', '#00600f'],   data=overall)\n",
    "ax.set_ylabel('')\n",
    "ax.tick_params(labelsize=24)\n",
    "ax.set_title('')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_style():\n",
    "    plt.style.use(['seaborn-white', 'seaborn-talk'])\n",
    "    plt.rc(\"font\", family=\"Times New Roman\", size = 30)\n",
    "    plt.rc('xtick', labelsize=30) \n",
    "    plt.rc('ytick', labelsize=30) \n",
    "set_style()  \n",
    "\n",
    "order = 'True Positive', 'True Negative', 'False Negative', 'False Positive'\n",
    "\n",
    "g = sns.factorplot(kind='box',        # Boxplot\n",
    "               y='value',       # Y-axis - values for boxplot\n",
    "               x='Metric',        # X-axis - first factor\n",
    "               hue = 'Metric',\n",
    "               #order= metric,\n",
    "               data=basics,        # Dataframe \n",
    "               size=8,            # Figure size (x100px)      \n",
    "               aspect=1.5, \n",
    "               palette= ['#01579b', '#0d47a1', '#c30000', '#c30000'],  # Width = size * aspect \n",
    "               legend_out=False)  # Make legend inside the plot\n",
    "#g.fig.get_axes()[0].set_yscale('log')\n",
    "\n",
    "g.fig.text(0.75, 0.75,'n = 117 floods', fontsize=30, color = '#ff3d00') #add text\n",
    "\n",
    "g.set_xticklabels(rotation=30)\n",
    "\n",
    "plt.ylabel('Count', fontsize = 45)\n",
    "plt.xlabel('', fontsize = 45)\n",
    "plt.title('', fontsize = 45)\n",
    "#plt.axhline(y=0, color='grey', ls='--')\n",
    "#plt.legend(loc='upper left', fontsize=45)\n",
    "\n",
    "%cd /Users/ckuhn/Desktop/gfd_accuracy_rename/figures/\n",
    "plt.savefig('GFD_basics.png', dpi = 300)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#http://queirozf.com/entries/matplotlib-pyplot-by-example#change-tick-label-rotation # ADD LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_style():\n",
    "    plt.style.use(['seaborn-white', 'seaborn-talk'])\n",
    "    plt.rc(\"font\", family=\"Times New Roman\", size = 30)\n",
    "    plt.rc('xtick', labelsize=30) \n",
    "    plt.rc('ytick', labelsize=30) \n",
    "set_style()  \n",
    "\n",
    "\n",
    "g = sns.factorplot(kind='bar',        # Boxplot\n",
    "               y='value',       # Y-axis - values for boxplot\n",
    "               x='Metric',        # X-axis - first factor\n",
    "               hue = 'Metric',\n",
    "               #order= metric,\n",
    "               data=overall,        # Dataframe \n",
    "               size=8,            # Figure size (x100px)      \n",
    "               aspect=1.5, \n",
    "               palette= ['#388e3c', '#6abf69', '#00600f'], \n",
    "               legend_out=False)  # Make legend inside the plot\n",
    "#g.fig.get_axes()[0].set_yscale('log')\n",
    "\n",
    "g.fig.text(0.2, 0.90,'n = 117 floods', fontsize=30, color = 'darkblue') #add text\n",
    "g.set_xticklabels(rotation=30)\n",
    "\n",
    "plt.ylabel('Score', fontsize = 45)\n",
    "plt.xlabel('', fontsize = 45)\n",
    "plt.title('', fontsize = 45)\n",
    "#plt.axhline(y=0, color='grey', ls='--')\n",
    "#plt.legend(loc='upper left', fontsize=45)\n",
    "\n",
    "\n",
    "\n",
    "%cd /Users/ckuhn/Desktop/gfd_accuracy_rename/figures/\n",
    "plt.savefig('GFD_overall_.png', dpi = 300)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_style():\n",
    "    plt.style.use(['seaborn-white', 'seaborn-talk'])\n",
    "    plt.rc(\"font\", family=\"Times New Roman\", size = 30)\n",
    "    plt.rc('xtick', labelsize=30) \n",
    "    plt.rc('ytick', labelsize=30) \n",
    "set_style()  \n",
    "\n",
    "\n",
    "g = sns.factorplot(kind='box',        # Boxplot\n",
    "               y='value',       # Y-axis - values for boxplot\n",
    "               x='Metric',        # X-axis - first factor\n",
    "               hue = 'Metric',\n",
    "               #order= metric,\n",
    "               data=error,        # Dataframe \n",
    "               size=8,            # Figure size (x100px)      \n",
    "               aspect=1.5, \n",
    "               palette= [ '#c30000','#c30000'],  # Width = size * aspect \n",
    "               legend_out=False)  # Make legend inside the plot\n",
    "#g.fig.get_axes()[0].set_yscale('log')\n",
    "\n",
    "g.fig.text(0.8, 0.8,'n = 117 floods', fontsize=30, color = 'darkblue') #add text\n",
    "\n",
    "plt.ylabel('Score', fontsize = 45)\n",
    "plt.xlabel('', fontsize = 45)\n",
    "plt.title('', fontsize = 45)\n",
    "#plt.axhline(y=0, color='grey', ls='--')\n",
    "#plt.legend(loc='upper left', fontsize=45)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# %cd /Users/ckuhn/Desktop/gfd_accuracy_rename/figures/\n",
    "# plt.savefig('GFD_error.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# q = df.loc[df['NumofPoints'] == 199]\n",
    "# x = df['NDVI'].values\n",
    "# y = df['overall_accuracy'].values\n",
    "# plt.plot(x, y, \"o\", color = 'green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NDVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to the scores of interest (excludes permanent water category)\n",
    "q = (inputData.loc[inputData['score']!= 2]) # permanent water\n",
    "q = (q.loc[q['score']!= 3]) \n",
    "q['score'].value_counts()\n",
    "\n",
    "#5 = true positive, 4 = true negative, 1 - false negative, 0 = true negative    \n",
    "params = {'axes.titlesize':'32',\n",
    "          'xtick.labelsize':'24',\n",
    "          'ytick.labelsize':'24'}\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "\n",
    "q.groupby(['score'])['NDVI'].mean().plot(kind='bar', color='grey')\n",
    "plt.xticks([0, 1, 2, 3], ['True Negative', 'False Negative', 'False Positive', 'True Positive'])\n",
    "plt.ylabel('Average NDVI', fontsize=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Sampling Points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of points for each flood\n",
    "fig, ax = plt.subplots(figsize=(15,7))\n",
    "df.groupby(['dfoID']).count()['score'].plot(ax=ax, kind='bar')\n",
    "ax.hlines(y=200, xmin=0, xmax=117, linewidth=2, color='r')\n",
    "ax.set_ylabel('Number of Points')\n",
    "ax.set_xlabel('DFO ID')\n",
    "ax.set_title('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count floods with fewer than 200 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.groupby('dfoID').filter(lambda g: g.dfoID.count() > 200)\n",
    "df2.dfoID.unique()\n",
    "\n",
    "p = df.groupby('dfoID').filter(lambda g: g.dfoID.count() < 100)\n",
    "p.dfoID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%cd '/Users/ckuhn/Desktop/gfd_accuracy_rename/lumped/'\n",
    "#df2.to_csv('all_student_data.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look into NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation\n",
      "-99.0     1589\n",
      " 0.0     12111\n",
      " 1.0     19550\n",
      " 99.0      557\n",
      "Name: dfoID, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(raw.groupby(['validation'])['dfoID'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total NA pixels 2146\n",
      "NA/total pixels 0.06285697548402214\n",
      "DFO IDs with NAs 105\n"
     ]
    }
   ],
   "source": [
    "drops1 = raw.loc[raw['validation'] < -1]\n",
    "drops2 = raw.loc[raw['validation'] > 98]\n",
    "drops = drops1.append(drops2)\n",
    "print('total NA pixels', len(drops))\n",
    "print('NA/total pixels', float(len(drops)/34141))\n",
    "print('DFO IDs with NAs', len(drops['dfoID'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dry/total pixels 0.3547347763685891\n",
      "water/total pixels 0.5726252892416742\n"
     ]
    }
   ],
   "source": [
    "dry = raw.loc[raw['validation'] == 0]\n",
    "water = raw.loc[raw['validation'] == 1]\n",
    "\n",
    "print('dry/total pixels', len(dry)/34141)\n",
    "print('water/total pixels', len(water)/34141)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>otsu_2day</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>2.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-99.0</th>\n",
       "      <td>459</td>\n",
       "      <td>350</td>\n",
       "      <td>776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99.0</th>\n",
       "      <td>179</td>\n",
       "      <td>112</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "otsu_2day   0.0  1.0  2.0\n",
       "validation               \n",
       "-99.0       459  350  776\n",
       " 99.0       179  112  266"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student = drops['validation'] \n",
    "model = drops['otsu_2day'] \n",
    "df_confusion = pd.crosstab(student, model)\n",
    "df_confusion\n",
    "# 0 = dry, 1 = permanent water, 2 = flood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>otsu_3day</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>2.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-99.0</th>\n",
       "      <td>547</td>\n",
       "      <td>350</td>\n",
       "      <td>688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99.0</th>\n",
       "      <td>216</td>\n",
       "      <td>112</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "otsu_3day   0.0  1.0  2.0\n",
       "validation               \n",
       "-99.0       547  350  688\n",
       " 99.0       216  112  229"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = drops['otsu_3day'] \n",
    "df_confusion = pd.crosstab(student, model)\n",
    "df_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>std_2day</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>2.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-99.0</th>\n",
       "      <td>389</td>\n",
       "      <td>350</td>\n",
       "      <td>846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99.0</th>\n",
       "      <td>166</td>\n",
       "      <td>112</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "std_2day    0.0  1.0  2.0\n",
       "validation               \n",
       "-99.0       389  350  846\n",
       " 99.0       166  112  279"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = drops['std_2day'] \n",
    "df_confusion = pd.crosstab(student, model)\n",
    "df_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>std_3day</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>2.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-99.0</th>\n",
       "      <td>399</td>\n",
       "      <td>350</td>\n",
       "      <td>836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99.0</th>\n",
       "      <td>168</td>\n",
       "      <td>112</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "std_3day    0.0  1.0  2.0\n",
       "validation               \n",
       "-99.0       399  350  836\n",
       " 99.0       168  112  277"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = drops['std_3day'] \n",
    "df_confusion = pd.crosstab(student, model)\n",
    "df_confusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:vectorenv]",
   "language": "python",
   "name": "conda-env-vectorenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
